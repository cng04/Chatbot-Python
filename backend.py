from io import BytesIO, StringIO
import io
import os
import openai
from openai import AzureOpenAI
from flask import Flask, request, jsonify, flash, redirect, url_for, abort
from flask_cors import CORS, cross_origin
import json
import textract
import fitz
import re
from os.path import join, dirname
from dotenv import load_dotenv
 
# Loading config file
dotenv_path = join(dirname(__file__), '.env')
load_dotenv(dotenv_path)
 
deployment_name= os.getenv("AZURE_OPENAI_MODEL")
azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")

app = Flask(__name__)
CORS(app, origins=["http://localhost:3000"], supports_credentials=True)

client = AzureOpenAI(
    base_url=f"{azure_endpoint}/openai/deployments/{deployment_name}/extensions",
    api_key=os.getenv("AZURE_OPENAI_KEY"),  
    api_version="2023-12-01-preview"  
)

### Old client
clientNoContext = AzureOpenAI(
  azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT"),
  api_key=os.getenv("AZURE_OPENAI_KEY"),  
  api_version="2023-12-01-preview"
)

# Holds the question/answer pairs the user indicated they are satisfied with
satisfiedResponses = {}

# Holds the question/answer pairs the user indicated they are unsatisfied with
unsatisfiedResponses = {}

# Process user reaction and adds the question the user asked and the answer the user either liked or disliked
# to the appropriate dictionary (for further use with DB)
@app.route('/userReaction', methods=['POST'])
def process_user_reaction():
    global satisfiedResponses
    global unsatisfiedResponses

    # Indicates whether this operation was successful
    result = {}

    try:
        request_data = json.loads(request.data)
        print(request_data)

        # Extracting user reaction object from request
        user_reaction_obj = request_data["userReactionRequest"]

        # Extracting user reaction (like or dislike) from user reaction object
        user_reaction = user_reaction_obj["userReaction"]

        # Extracting previous question user asked
        prev_question = user_reaction_obj["question"]

        # Extracting answer that user either liked or disliked
        prev_answer = user_reaction_obj["answer"]

        # Adding question/answer pairs to the appropriate dictionaries
        if (user_reaction == "like"):
            satisfiedResponses[prev_question] = prev_answer
        elif (user_reaction == "dislike"):
            unsatisfiedResponses[prev_question] = prev_answer

        # for key, value in satisfiedResponses.items():
        #     print(f"key: {key}, value: {value}")

        # for key, value in unsatisfiedResponses.items():
        #     print(f"key: {key}, value: {value}")

        # Successful operation
        result["status"] = 1
    except:
        # Unsuccessful operation
        result["status"] = 0
    
    return jsonify(result)

# Process new question from UI, send to Open AI model to generate answer, then send answer back to UI
@app.route('/question', methods=['POST'])
def generate_response():
    global all_text
    query = "Please give a concise and uncontradictory answer to the following question: "

    # Loading and extracting message
    # print(request.data)

    request_data = json.loads(request.data)

    # print(request_data)

    question = request_data["content"]

    # print(question)

    # If user HAS uploaded a document, then provide the index for the model to use to answer
    if (all_text != ""):
        print("User has uploaded a document")

        query = f"""
        Please use the text below to answer the subsequent question. If the answer cannot be found, write
        "I don't know"
        {all_text}
        """

    apiMessage = [{
    "role": "user",
    "content": query + question["message"]
    }]

    # print(question["message"])

    # Variable to hold the similar questions generated by the model (if any)
    similarQuestionsResult = []

    # Invoking model to generate response
    response = client.chat.completions.create(model=deployment_name, messages=apiMessage,  
                                            extra_body={
                                                "dataSources": [  # camelCase is intentional, as this is the format the API expects
                                                    {
                                                        "type": "AzureCognitiveSearch",
                                                        "parameters": {
                                                            "endpoint": os.environ.get("AZURE_AI_SEARCH_ENDPOINT"),
                                                            "key": os.environ.get("AZURE_AI_SEARCH_API_KEY"),
                                                            "indexName": os.environ.get("AZURE_AI_SEARCH_INDEX"),
                                                        }
                                                    }
                                                ]  
                                            })
    
    # Extracting specifically the response the model generated
    response_message = response.choices[0].message.content

    print(response_message)

    # Removing any [doc *] content from end of response_message
    clean_response_message = re.sub(r"\[doc\s*\d+\]", "", response_message)

    print(clean_response_message)

    # Assigning the list of similar questions the model generated to similarQuestionsResult
    similarQuestionsResult = search_for_similar_questions(clean_response_message)

    # Formatting answer - similar-questions holds each similar question, answer holds the OpenAI model response to the question asked
    response = {"similarQuestions": similarQuestionsResult, # this will eventually hold the similar-questions generated by OpenAI AI Search
                    "answer": {
                        "sender": "Azure Open AI",
                        "message": clean_response_message
                    }
                }

    # Returning answer
    return jsonify(response)

# Asking the model to find similar questions to the question the user asked
def search_for_similar_questions(question):
    result = []

    query = "Please generate only four similar questions and return just the question text, no question number or extra symbols to the following question: "

    apiMessage = [{
        "role": "user",
        "content": query + question
    }]

    print(apiMessage)

     # Invoking model to generate response
    response = client.chat.completions.create(model=deployment_name, messages=apiMessage,  
                                              extra_body={
                                                "dataSources": [  # camelCase is intentional, as this is the format the API expects
                                                    {
                                                        "type": "AzureCognitiveSearch",
                                                        "parameters": {
                                                            "endpoint": os.environ.get("AZURE_AI_SEARCH_ENDPOINT"),
                                                            "key": os.environ.get("AZURE_AI_SEARCH_API_KEY"),
                                                            "indexName": os.environ.get("AZURE_AI_SEARCH_INDEX"),
                                                        }
                                                    }
                                                ]  
                                              })
    
    similarQuestions = response.choices[0].message.content

    similarQuestionsSplit = similarQuestions.split("\n")

    print(similarQuestionsSplit)

    # Extracting each question and storing them in a global variable in sq{index}
    for index, question in enumerate(similarQuestionsSplit):
        globals()[f"sq{index}"] = re.sub(r"\[doc\s*\d+\]", "", question.lstrip("- ")).strip()

    for index in range(len(similarQuestionsSplit)):
        print(globals()[f"sq{index}"])

    # Adding each separate similar question to an object which is then appended to result which
    # will be returned
        
    ''' If model didn't generate any questions, then return an empty list []. The React UI
        has a state variable to handle if the returned list from the Python is empty, 
        then similar questions is not generated
    '''
    for index in range(len(similarQuestionsSplit)):
        # Ensuring that similar questions with these phrases are not returned to the user, only actual relevant similar questions
        if ("I'm sorry, I cannot fulfill that request" not in globals()[f"sq{index}"] and "### Similar Question" not in globals()[f"sq{index}"]
            and "Similar Question" not in globals()[f"sq{index}"] and "similar question" not in globals()[f"sq{index}"] and globals()[f"sq{index}"] != ""
            and "out-of-domain" not in globals()[f"sq{index}"] and "questions that are similar" not in globals()[f"sq{index}"]
            and "not available" not in globals()[f"sq{index}"] and "not suppported" not in globals()[f"sq{index}"] and "" != globals()[f"sq{index}"]
            and "fulfill this request." not in globals()[f"sq{index}"]):
            eachQuestion = {
                "question": {
                    "sender": "Open AI",
                    "message": globals()[f"sq{index}"]
                }
            }

            result.append(eachQuestion)

    print(result)

    return result

# Global variable to store pdf text
all_text = ""

# UI uploading pdf document
@app.route('/upload', methods=['POST'])
def upload():
    global all_text

    print(request.files["file"])

    # Dict to hold status result of reading the file
    result = {}

    try:

        file = request.files["file"]

        # Retrieving filename
        filename = file.filename
        print(f"Uploading file {filename}")

        file_bytes = file.read()

        # Converting File Object to Bytes Object
        file_content = BytesIO(file_bytes).getvalue()

        # Extracting string from Bytes Object if file is pdf
        if filename[-4:] == ".pdf":
            try:
                doc = fitz.open(stream=file_content, filetype="pdf")

                for page in doc:
                    all_text += page.get_text('text')

                print(all_text)
            except Exception as e:
                print("Error is " + e)

        # Setting status to be 1
        result["status"] = 1
    except Exception as e:
        print(f"Couldn't upload file {e}")
        result["status"] = 0

    # if result["status"] == 1:
    #     query_with_context()

    # Returning result dict to React UI
    # jsonify turns the JSON output into a Response object with the application/json mimetype
    return jsonify(result)

# Question to handle the summarize button click and summarize with context input
@app.route('/summarize', methods=['POST'])
def summarize():
    request_data = json.loads(request.data)

    print(request_data)

    # Extracting the context text
    context_text = request_data["contextRequest"]["context"]

    # Getting the model's response for the summary/summary with context
    summary = generate_summary(context_text)

    result = {"response": summary}

    return jsonify(result)
        
# Function to generate summary (optionally with provided context)
def generate_summary(context):
    prompt = ""

    if (context == ""):
        prompt = "Please summarize the text."
    else:
        prompt = context

    query = f"""
        Please use the text below to answer the subsequent question. If the answer cannot be found, write
        "I don't know"
        {all_text}
    """ + prompt

    # Invoking model to generate response
    response = clientNoContext.chat.completions.create(model=deployment_name, messages=[{"role": "user", "content": query}])

    print("The response from querying the index is: " + response.choices[0].message.content)

    return response.choices[0].message.content


# Simple function to test out providing the model an index to query from
def query_with_context():
    question = "What is an example of a rivalry in the NBA"
    query = f"""
        Please use the text below to answer the subsequent question. If the answer cannot be found, write
        "I don't know"
        {all_text}
    """ + question

    # Invoking model to generate response
    response = client.chat.completions.create(model=deployment_name, messages=[{"role": "user", "content": query}])

    print("The response from querying the index is: " + response.choices[0].message.content)

# Route to clear all_text (index that stores text from pdfs)
@app.route('/clear', methods=['POST'])
def clear():
    global all_text
    all_text = ""

    result = {}
    result["status"] = "cleared"

    return jsonify(result)

# Endpoint to test returning "Similar Questions"
@app.route('/similar', methods=['POST'])
def similar():

    result = {"similarQuestions": [
        {
            "question": 
                {
                    "sender": "Open AI",
                    "message": "Question 1"
                }
        },
        {
            "question": 
                {
                    "sender": "Open AI",
                    "message": "Question 2"
                }
        },
        {
            "question": 
                {
                    "sender": "Open AI",
                    "message": "Question 3"
                }
        },
        {
            "question": 
                {
                    "sender": "Open AI",
                    "message": "Question 4"
                }
        }], "answer": {
                    "sender": "Open AI",
                    "message": "Hehe"
                }
    }

    return jsonify(result)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8081, debug=True)